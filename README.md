# JuniorDesignApp

## Release Notes

## Installation Guide
### Install App
git clone https://github.com/Erickkbentz/JuniorDesignApp.git

cd JuniorDesignApp

git submodule init

git submodule update



### -- Setup Frontend and Prisma database client --
cd JuniorDesignFE
npm install

npm i -g prisma@latest

npm i @prisma/client@latest

prisma generate



### -- Setup Machine Learning --
cd ../JuniorDesignML

pip install Flask praw pandas numpy PyPDF2 nltk text2emotion datetime

python app.py

NOTE: If running into any bugs 'module xxx does not exist' run 'pip install xxx'

## Machine Learning Info

### -- Stage 1: Persuasion Detection --
For the persuasion detection file (located at JuniorDesignML/Models/PersuasionDetector/modelv3.ipyynb) we are doing a supervised text classification algorithm. We are primarily using the nltk, panas, and numpy libraries for data setup, preprocessing, and processing. To train the model, we used a test set generated by PRAW (using prawScript in the /OldOrTest folder). For persuasive examples we scraped the post bodies of r/UnpopularOpinion, and the comments of r/AmITheAsshole and r/ChangeMyView. For non persuasive examples we scraped the post comments from r/Gaming, r/WritingPrompts, and the post bodies of r/NoSleep. From there we created three feature sets (a count vector, a word vector, and an n-gram vector), and trained them on two different model types (Naive-Bayes and Linear Regression). At the bottom of the file we run each of the 6 models created against a manually created test set to gauge their accuracies. We found the Naive-Bayes Count Vector model performed best so that is the one being used in the dashboard currently. The file is set up so any new data set can be inputted and used to retrain the model in an attempt to increase its accuracy. 

### -- Stage 2: Persuasion Classification --


We tried multiple strategies to utilize a k-means clustering algorithm to classify persuasion. Nothing we did seemed to make the resulting clusters less random. We tried introducing bias by searching for keywords within the text that might indicate logos, ethos, or pathos, and concattonated "logos", "ethos", "pathos" to the end of the string accordingly. (i.e. if it is rainy, then you need an umbrella logos). We thought it would help the allgorithm better find a pattern within texts containing keywords. This did not work.
